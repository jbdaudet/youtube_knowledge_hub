{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pytube\n",
    "import speech_recognition as sr\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from pydub import AudioSegment\n",
    "#Local version of the file containing the secret key\n",
    "import gitignore as g\n",
    "\n",
    "pd.set_option('max_colwidth', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Set up OpenAI API key\n",
    "openai_api_key = g.OPENAI_API_KEY\n",
    "client = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List all videos from a youtube channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "\n",
    "def get_channel_videos(api_key, channel_id):\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    # Get the channel's uploads playlist ID\n",
    "    request = youtube.channels().list(\n",
    "        part='contentDetails',\n",
    "        id=channel_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "    \n",
    "    uploads_playlist_id = response['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "    \n",
    "    # Get videos from the uploads playlist\n",
    "    videos = []\n",
    "    next_page_token = None\n",
    "    \n",
    "    while True:\n",
    "        request = youtube.playlistItems().list(\n",
    "            part='snippet',\n",
    "            playlistId=uploads_playlist_id,\n",
    "            maxResults=50,\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        response = request.execute()\n",
    "        \n",
    "        for item in response['items']:\n",
    "            video_id = item['snippet']['resourceId']['videoId']\n",
    "            video_url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "            videos.append(video_url)\n",
    "        \n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if next_page_token is None:\n",
    "            break\n",
    "    \n",
    "    return videos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_videos_urls = get_channel_videos(api_key = g.YOUTUBE_API_KEY, channel_id = 'UCbZdXox6mKHdcT2QdVT-goQ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful sound treatment functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful sound treatment functions\n",
    "\n",
    "\n",
    "def download_youtube_video(video_url, output_path='output'):\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    \n",
    "    yt = pytube.YouTube(video_url)\n",
    "    video = yt.streams.filter(only_audio=True).first()\n",
    "    output_file = video.download(output_path)\n",
    "    return output_file\n",
    "\n",
    "def convert_audio_to_wav(audio_file, output_format='wav'):\n",
    "    audio = AudioSegment.from_file(audio_file)\n",
    "    wav_file = f\"{os.path.splitext(audio_file)[0]}.{output_format}\"\n",
    "    audio.export(wav_file, format=output_format)\n",
    "    return wav_file\n",
    "\n",
    "def transcribe_audio_chunk(audio_chunk):\n",
    "    try:\n",
    "        response = client.audio.transcriptions.create(\n",
    "                model=\"whisper-1\", file=audio_chunk)\n",
    "        text = response.text\n",
    "\n",
    "        return text\n",
    "    except sr.UnknownValueError:\n",
    "        return \"(Unintelligible)\"\n",
    "    except sr.RequestError as e:\n",
    "        return f\"(RequestError: {e})\"\n",
    "\n",
    "\n",
    "def transcribe_audio(audio_file, video_url, chunk_length=30000):\n",
    " \n",
    "    audio = AudioSegment.from_wav(audio_file)\n",
    "    transcriptions = []\n",
    "\n",
    "    for i in range(0, len(audio), chunk_length):\n",
    "        start_time = i / 1000  # Convert to seconds\n",
    "        audio_chunk = audio[i:i + chunk_length]\n",
    "        audio_chunk_wav = f\"temp_chunk_{i}.wav\"\n",
    "        audio_chunk.export(audio_chunk_wav, format=\"wav\")\n",
    "        with open(audio_chunk_wav, \"rb\") as audio_file:\n",
    "            text = transcribe_audio_chunk( audio_file)\n",
    "            transcriptions.append({'start_time': start_time, 'text': text, 'video_url': video_url})\n",
    "\n",
    "        # Remove the temporary WAV file\n",
    "        os.remove(audio_chunk_wav)\n",
    "    \n",
    "    return transcriptions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capture all videos and extract text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Videos to process: 447\n"
     ]
    }
   ],
   "source": [
    "print('Videos to process:', len(youtube_videos_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video n° 1: https://www.youtube.com/watch?v=4dAGl7RLWHc\n",
      "Processing video n° 2: https://www.youtube.com/watch?v=W70rkratv0Q\n",
      "Processing video n° 3: https://www.youtube.com/watch?v=9NUXbsiXo4U\n",
      "Processing video n° 4: https://www.youtube.com/watch?v=MzTzZissnac\n",
      "Processing video n° 5: https://www.youtube.com/watch?v=gi4m2MODsCA\n",
      "Processing video n° 6: https://www.youtube.com/watch?v=CYYqjl9wRUM\n",
      "Processing video n° 7: https://www.youtube.com/watch?v=AroRz9r9MLw\n",
      "Processing video n° 8: https://www.youtube.com/watch?v=CMuSBWWp9Qc\n",
      "Processing video n° 9: https://www.youtube.com/watch?v=zzKFJEPzZUM\n",
      "Processing video n° 10: https://www.youtube.com/watch?v=YffxKI0Uwl0\n",
      "Processing video n° 11: https://www.youtube.com/watch?v=w3HaMoO6j3g\n",
      "Processing video n° 12: https://www.youtube.com/watch?v=qWpyKxVqNMk\n",
      "Processing video n° 13: https://www.youtube.com/watch?v=bicaJPdpqIE\n",
      "Processing video n° 14: https://www.youtube.com/watch?v=GorutlUSDFs\n",
      "Processing video n° 15: https://www.youtube.com/watch?v=JUdee1pphlU\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main(video_urls):\n",
    "    i=0\n",
    "    for video_url in video_urls:\n",
    "        i = i+1\n",
    "        try:\n",
    "            print(f\"Processing video n° {i}: {video_url}\")\n",
    "            audio_file = download_youtube_video(video_url)\n",
    "            wav_file = convert_audio_to_wav(audio_file)\n",
    "\n",
    "            transcriptions = transcribe_audio(wav_file, video_url)\n",
    "            \n",
    "            # Extract video ID from the URL\n",
    "            video_id = video_url.split('v=')[1]\n",
    "            individual_csv_file = f'transcriptions_{video_id}.csv'\n",
    "            \n",
    "            # Save the individual transcription file\n",
    "            df = pd.DataFrame(transcriptions)\n",
    "            df.to_csv(individual_csv_file, index=False)\n",
    "            \n",
    "            # Append to the main transcription file\n",
    "            with open(individual_csv_file, 'r') as ind_file:\n",
    "                if not os.path.exists('transcriptions.csv'):\n",
    "                    with open('transcriptions.csv', 'w') as main_file:\n",
    "                        main_file.write(ind_file.read())\n",
    "                else:\n",
    "                    with open('transcriptions.csv', 'a') as main_file:\n",
    "                        next(ind_file)  # Skip header row\n",
    "                        main_file.write(ind_file.read())\n",
    "            \n",
    "            # Remove the temporary files\n",
    "            os.remove(audio_file)\n",
    "            os.remove(wav_file)\n",
    "            os.remove(individual_csv_file)\n",
    "        except: \n",
    "            print('This video can not be captured')\n",
    "            pass\n",
    "    print(\"Transcription completed and saved to individual and main CSV files.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # List of YouTube video URLs to process\n",
    "  \n",
    "    main(youtube_videos_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add a summary of the chunk of text with chatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text):\n",
    "    while True:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4-turbo\",\n",
    "                messages=[{'role':'user', 'content':f\"sum up in one sentence the following text: {text}\"}])\n",
    "            return response.choices[0].message.content\n",
    "\n",
    "        except:\n",
    "            print(\"let's have a 10sec. nap, shall we?\")\n",
    "            time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries added and saved to transcriptions_with_summary.csv.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def add_summaries_to_transcriptions(file_in, file_out):\n",
    "    # Read the transcriptions CSV file\n",
    "    df = pd.read_csv(file_in)\n",
    "\n",
    "    # Apply the summarization function to each row in the DataFrame\n",
    "    df['summary'] = df['text'].apply(summarize_text)\n",
    "\n",
    "    # Save the updated DataFrame back to the CSV file\n",
    "    df.to_csv(file_out, index=False)\n",
    "    print(f\"Summaries added and saved to {file_out}.\")\n",
    "\n",
    "add_summaries_to_transcriptions('transcriptions.csv', 'transcriptions_with_summary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### end of script"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pampers-utils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
